# Databricks notebook source
# MAGIC %md
# MAGIC # Exercise 6: MLflowã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨å®Ÿé¨“ç®¡ç†
# MAGIC
# MAGIC ## ç›®çš„
# MAGIC - MLflowã‚’ä½¿ã£ãŸå®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¨ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚’å­¦ã¶
# MAGIC - LLMã®è©•ä¾¡æŒ‡æ¨™ï¼ˆBLEUã€ROUGEã€BERTScoreï¼‰ã‚’è‡ªå‹•è¨˜éŒ²
# MAGIC - MLflow Model Registryã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
# MAGIC - Databricks AutoMLã¨ã®é€£æºã«ã‚ˆã‚‹è©•ä¾¡è‡ªå‹•åŒ–
# MAGIC - MLflow Evaluate APIã§LLMè©•ä¾¡ã‚’æ¨™æº–åŒ–
# MAGIC
# MAGIC ## Databricks + MLflowã®åˆ©ç‚¹
# MAGIC - å®Ÿé¨“ã®è‡ªå‹•ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
# MAGIC - ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°
# MAGIC - è©•ä¾¡æŒ‡æ¨™ã®å¯è¦–åŒ–ã¨æ¯”è¼ƒ
# MAGIC - æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã¸ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªç§»è¡Œ

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

# COMMAND ----------

# DBTITLE 1,å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# MAGIC %pip install --upgrade transformers datasets evaluate rouge-score bert-score sacrebleu nltk peft torch mlflow
# MAGIC
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,MLflowã¨ç’°å¢ƒã®è¨­å®š
import mlflow
import mlflow.transformers
from mlflow.models.signature import infer_signature
import torch
import transformers

# MLflowå®Ÿé¨“ã®è¨­å®š
username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()
experiment_name = f"/Users/{username}/llm-finetuning-evaluation"

mlflow.set_experiment(experiment_name)

print(f"ã€MLflowå®Ÿé¨“è¨­å®šã€‘")
print(f"å®Ÿé¨“å: {experiment_name}")
print(f"å®Ÿé¨“ID: {mlflow.get_experiment_by_name(experiment_name).experiment_id}")
print(f"\nã€ç’°å¢ƒæƒ…å ±ã€‘")
print(f"transformers: {transformers.__version__}")
print(f"mlflow: {mlflow.__version__}")
print(f"torch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨MLflowã¸ã®ç™»éŒ²

# COMMAND ----------

# DBTITLE 1,ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨MLflow Datasetsã¸ã®ç™»éŒ²
from datasets import load_dataset
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
dataset = load_dataset("bbz662bbz/databricks-dolly-15k-ja-gozarinnemon", split="train")
dataset = dataset.train_test_split(test_size=0.2, seed=42)
test_dataset = dataset['test']

# è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
import random
random.seed(42)
eval_indices = random.sample(range(len(test_dataset)), min(100, len(test_dataset)))
eval_dataset = test_dataset.select(eval_indices)

# DataFrameã«å¤‰æ›ï¼ˆMLflowç”¨ï¼‰
eval_data = []
for sample in eval_dataset:
    prompt = sample['instruction']
    if sample.get('input') and sample['input'].strip():
        prompt = f"{prompt}\n\n{sample['input']}"
    eval_data.append({
        'prompt': prompt,
        'ground_truth': sample['output']
    })

eval_df = pd.DataFrame(eval_data)

# MLflow Datasetsã¨ã—ã¦ç™»éŒ²
dataset_source = mlflow.data.from_pandas(
    eval_df,
    source="bbz662bbz/databricks-dolly-15k-ja-gozarinnemon",
    name="evaluation_dataset"
)

print(f"âœ… è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(eval_df)}ã‚µãƒ³ãƒ—ãƒ«")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ï¼ˆMLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼‰

# COMMAND ----------

# DBTITLE 1,ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨æ¨è«–
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

base_model_id = "google/gemma-3-270m-it"

def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 150) -> str:
    """ãƒ¢ãƒ‡ãƒ«ã§å¿œç­”ã‚’ç”Ÿæˆ"""
    messages = [{"role": "user", "content": prompt}]
    
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return response

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
print("ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã€‘")
base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
print("âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")

# COMMAND ----------

# DBTITLE 1,ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨MLflowã¸ã®è¨˜éŒ²
import evaluate
from tqdm import tqdm
import time

# è©•ä¾¡æŒ‡æ¨™ã®ãƒ­ãƒ¼ãƒ‰
bleu_metric = evaluate.load("sacrebleu")
rouge_metric = evaluate.load("rouge")

def evaluate_model_with_mlflow(model, tokenizer, eval_df, model_name: str, run_name: str):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€çµæœã‚’MLflowã«è¨˜éŒ²
    """
    # MLflow Runã®é–‹å§‹
    with mlflow.start_run(run_name=run_name) as run:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ã‚°
        mlflow.log_input(dataset_source, context="evaluation")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ãƒ­ã‚°
        mlflow.log_param("model_name", model_name)
        mlflow.log_param("num_parameters", model.num_parameters())
        mlflow.log_param("eval_samples", len(eval_df))
        mlflow.log_param("max_new_tokens", 150)
        mlflow.log_param("temperature", 0.7)
        
        # æ¨è«–ã®å®Ÿè¡Œ
        predictions = []
        references = []
        inference_times = []
        
        print(f"ã€{model_name}ã®è©•ä¾¡é–‹å§‹ã€‘")
        
        for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc="Generating"):
            start_time = time.time()
            prediction = generate_response(model, tokenizer, row['prompt'])
            inference_time = time.time() - start_time
            
            predictions.append(prediction)
            references.append(row['ground_truth'])
            inference_times.append(inference_time)
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        # BLEU
        bleu_result = bleu_metric.compute(
            predictions=predictions,
            references=[[ref] for ref in references]
        )
        
        # ROUGE
        rouge_result = rouge_metric.compute(
            predictions=predictions,
            references=references,
            use_stemmer=False
        )
        
        # BERTScore
        from bert_score import score as bert_score
        P, R, F1 = bert_score(
            predictions,
            references,
            lang="ja",
            verbose=False,
            model_type="cl-tohoku/bert-base-japanese-v3"
        )
        
        # ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™: ã”ã–ã‚‹å£èª¿ä½¿ç”¨ç‡
        gozaru_count = sum(1 for pred in predictions if any(word in pred for word in ['ã”ã–ã‚‹', 'ã”ã•ã„ã¾ã™', 'ã”ã–ã„ã¾ã™']))
        gozaru_rate = gozaru_count / len(predictions) * 100
        
        # å¿œç­”é•·
        avg_pred_length = sum(len(p) for p in predictions) / len(predictions)
        avg_ref_length = sum(len(r) for r in references) / len(references)
        
        # å¹³å‡æ¨è«–æ™‚é–“
        avg_inference_time = sum(inference_times) / len(inference_times)
        
        # MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
        mlflow.log_metric("bleu_score", bleu_result['score'])
        mlflow.log_metric("rouge1_score", rouge_result['rouge1'])
        mlflow.log_metric("rouge2_score", rouge_result['rouge2'])
        mlflow.log_metric("rougeL_score", rouge_result['rougeL'])
        mlflow.log_metric("bertscore_precision", P.mean().item())
        mlflow.log_metric("bertscore_recall", R.mean().item())
        mlflow.log_metric("bertscore_f1", F1.mean().item())
        mlflow.log_metric("gozaru_style_rate", gozaru_rate)
        mlflow.log_metric("avg_prediction_length", avg_pred_length)
        mlflow.log_metric("avg_reference_length", avg_ref_length)
        mlflow.log_metric("length_difference", abs(avg_pred_length - avg_ref_length))
        mlflow.log_metric("avg_inference_time_sec", avg_inference_time)
        
        # çµæœã®DataFrameã‚’ä½œæˆ
        results_df = pd.DataFrame({
            'prompt': eval_df['prompt'].tolist(),
            'ground_truth': references,
            'prediction': predictions,
            'inference_time': inference_times
        })
        
        # çµæœã‚’CSVã¨ã—ã¦ãƒ­ã‚°
        results_df.to_csv("/tmp/predictions.csv", index=False)
        mlflow.log_artifact("/tmp/predictions.csv", "predictions")
        
        # ã‚µãƒ³ãƒ—ãƒ«çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦ãƒ­ã‚°
        sample_results = results_df.head(10)
        mlflow.log_table(sample_results, "sample_predictions.json")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’MLflowã«ç™»éŒ²
        print("\nãƒ¢ãƒ‡ãƒ«ã‚’MLflowã«ç™»éŒ²ä¸­...")
        mlflow.transformers.log_model(
            transformers_model={
                "model": model,
                "tokenizer": tokenizer
            },
            artifact_path="model",
            task="text-generation",
            registered_model_name=f"gemma-3-270m-{run_name.replace(' ', '-')}"
        )
        
        print(f"\nâœ… è©•ä¾¡å®Œäº†")
        print(f"Run ID: {run.info.run_id}")
        print(f"BLEU: {bleu_result['score']:.2f}")
        print(f"ROUGE-L: {rouge_result['rougeL']:.4f}")
        print(f"BERTScore F1: {F1.mean().item():.4f}")
        print(f"ã”ã–ã‚‹å£èª¿ä½¿ç”¨ç‡: {gozaru_rate:.1f}%")
        
        return {
            'run_id': run.info.run_id,
            'predictions': predictions,
            'references': references,
            'metrics': {
                'bleu': bleu_result['score'],
                'rouge1': rouge_result['rouge1'],
                'rouge2': rouge_result['rouge2'],
                'rougeL': rouge_result['rougeL'],
                'bertscore_f1': F1.mean().item(),
                'gozaru_rate': gozaru_rate
            }
        }

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
base_results = evaluate_model_with_mlflow(
    base_model,
    base_tokenizer,
    eval_df,
    base_model_id,
    "base-model"
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

# COMMAND ----------

# DBTITLE 1,ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨è©•ä¾¡
from peft import AutoPeftModelForCausalLM

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
finetuned_model_path = "/dbfs/tmp/gemma-3-270m-lora-adapters"

print("ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã€‘")
finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)
finetuned_model = AutoPeftModelForCausalLM.from_pretrained(
    finetuned_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
finetuned_results = evaluate_model_with_mlflow(
    finetuned_model,
    finetuned_tokenizer,
    eval_df,
    f"{base_model_id} + LoRA",
    "finetuned-model-lora"
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 4: MLflow UIã§ã®æ¯”è¼ƒ

# COMMAND ----------

# DBTITLE 1,å®Ÿé¨“çµæœã®æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
# MLflow APIã§å®Ÿé¨“çµæœã‚’å–å¾—
experiment = mlflow.get_experiment_by_name(experiment_name)
runs = mlflow.search_runs(
    experiment_ids=[experiment.experiment_id],
    order_by=["start_time DESC"],
    max_results=10
)

# ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
comparison_df = runs[[
    'run_id',
    'tags.mlflow.runName',
    'params.model_name',
    'metrics.bleu_score',
    'metrics.rougeL_score',
    'metrics.bertscore_f1',
    'metrics.gozaru_style_rate',
    'metrics.avg_inference_time_sec',
    'start_time'
]].copy()

comparison_df.columns = [
    'Run ID',
    'Run Name',
    'Model',
    'BLEU',
    'ROUGE-L',
    'BERTScore F1',
    'ã”ã–ã‚‹ç‡ (%)',
    'æ¨è«–æ™‚é–“ (ç§’)',
    'å®Ÿè¡Œæ—¥æ™‚'
]

print("ã€å®Ÿé¨“çµæœã®æ¯”è¼ƒã€‘")
display(comparison_df.head())

# COMMAND ----------

# DBTITLE 1,ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt
import numpy as np

# æœ€æ–°ã®2ã¤ã®Runã‚’æ¯”è¼ƒ
latest_runs = comparison_df.head(2)

if len(latest_runs) >= 2:
    metrics = ['BLEU', 'ROUGE-L', 'BERTScore F1', 'ã”ã–ã‚‹ç‡ (%)']
    base_values = [
        latest_runs.iloc[1]['BLEU'],
        latest_runs.iloc[1]['ROUGE-L'],
        latest_runs.iloc[1]['BERTScore F1'],
        latest_runs.iloc[1]['ã”ã–ã‚‹ç‡ (%)']
    ]
    finetuned_values = [
        latest_runs.iloc[0]['BLEU'],
        latest_runs.iloc[0]['ROUGE-L'],
        latest_runs.iloc[0]['BERTScore F1'],
        latest_runs.iloc[0]['ã”ã–ã‚‹ç‡ (%)']
    ]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ã‚°ãƒ©ãƒ•1: ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
    x = np.arange(len(metrics))
    width = 0.35
    
    axes[0].bar(x - width/2, base_values, width, label='ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«', alpha=0.8)
    axes[0].bar(x + width/2, finetuned_values, width, label='ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿', alpha=0.8)
    axes[0].set_xlabel('è©•ä¾¡æŒ‡æ¨™')
    axes[0].set_ylabel('ã‚¹ã‚³ã‚¢')
    axes[0].set_title('è©•ä¾¡æŒ‡æ¨™åˆ¥ã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(metrics, rotation=45, ha='right')
    axes[0].legend()
    axes[0].grid(axis='y', alpha=0.3)
    
    # ã‚°ãƒ©ãƒ•2: æ”¹å–„ç‡
    improvement = [(ft - base) / base * 100 for ft, base in zip(finetuned_values, base_values)]
    colors = ['green' if x > 0 else 'red' for x in improvement]
    
    axes[1].barh(metrics, improvement, color=colors, alpha=0.7)
    axes[1].set_xlabel('æ”¹å–„ç‡ (%)')
    axes[1].set_title('ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹æ”¹å–„ç‡')
    axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)
    axes[1].grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("/tmp/comparison_chart.png", dpi=150, bbox_inches='tight')
    plt.show()
    
    # ã‚°ãƒ©ãƒ•ã‚’MLflowã«ãƒ­ã‚°
    with mlflow.start_run(run_id=finetuned_results['run_id']):
        mlflow.log_artifact("/tmp/comparison_chart.png", "charts")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 5: MLflow Evaluateã‚’ä½¿ã£ãŸæ¨™æº–åŒ–ã•ã‚ŒãŸè©•ä¾¡

# COMMAND ----------

# DBTITLE 1,MLflow Evaluateã«ã‚ˆã‚‹è©•ä¾¡
# MLflow 2.8ä»¥é™ã§åˆ©ç”¨å¯èƒ½ãªLLMè©•ä¾¡æ©Ÿèƒ½

def create_evaluation_dataset():
    """MLflow Evaluateç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    eval_data_for_mlflow = []
    for idx, row in eval_df.iterrows():
        eval_data_for_mlflow.append({
            "inputs": row['prompt'],
            "ground_truth": row['ground_truth']
        })
    return pd.DataFrame(eval_data_for_mlflow)

# è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
mlflow_eval_data = create_evaluation_dataset()

# ãƒ¢ãƒ‡ãƒ«ã‚’PyFuncã¨ã—ã¦ãƒ©ãƒƒãƒ—
class GemmaModelWrapper(mlflow.pyfunc.PythonModel):
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def predict(self, context, model_input):
        """ãƒãƒƒãƒäºˆæ¸¬"""
        if isinstance(model_input, pd.DataFrame):
            prompts = model_input['inputs'].tolist()
        else:
            prompts = model_input
        
        predictions = []
        for prompt in prompts:
            response = generate_response(self.model, self.tokenizer, prompt)
            predictions.append(response)
        
        return predictions

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—
wrapped_model = GemmaModelWrapper(finetuned_model, finetuned_tokenizer)

# ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™ã®å®šç¾©
from mlflow.metrics import make_metric

def gozaru_style_score(eval_df, builtin_metrics):
    """ã”ã–ã‚‹å£èª¿ã®ä½¿ç”¨ç‡ã‚’è©•ä¾¡"""
    predictions = eval_df['predictions'].tolist()
    gozaru_count = sum(1 for pred in predictions if any(word in pred for word in ['ã”ã–ã‚‹', 'ã”ã•ã„ã¾ã™', 'ã”ã–ã„ã¾ã™']))
    return gozaru_count / len(predictions)

gozaru_metric = make_metric(
    eval_fn=gozaru_style_score,
    greater_is_better=True,
    name="gozaru_style_consistency"
)

# MLflow Evaluateã§è©•ä¾¡
print("ã€MLflow Evaluateã«ã‚ˆã‚‹è©•ä¾¡ã€‘")

with mlflow.start_run(run_name="mlflow-evaluate-finetuned") as run:
    results = mlflow.evaluate(
        model=wrapped_model,
        data=mlflow_eval_data,
        targets="ground_truth",
        model_type="text",
        extra_metrics=[gozaru_metric],
        evaluators="default"
    )
    
    print("\nâœ… MLflow Evaluateå®Œäº†")
    print(f"Run ID: {run.info.run_id}")
    print("\nã€è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘")
    for metric_name, metric_value in results.metrics.items():
        print(f"  {metric_name}: {metric_value}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 6: LLM-as-a-Judge with MLflow

# COMMAND ----------

# DBTITLE 1,LLM-as-a-Judgeã®å®Ÿè£…ã¨MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
from databricks.sdk import WorkspaceClient

w = WorkspaceClient()
judge_client = w.serving_endpoints.get_open_ai_client()

def llm_as_judge_batch(prompts, references, candidates, sample_size=10):
    """
    ãƒãƒƒãƒã§LLM-as-a-Judgeè©•ä¾¡ã‚’å®Ÿè¡Œ
    """
    scores = []
    
    for i in range(min(sample_size, len(prompts))):
        judge_prompt = f"""ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’5æ®µéšã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{prompts[i]}

ã€å‚ç…§å›ç­”ã€‘
{references[i]}

ã€è©•ä¾¡å¯¾è±¡ã®å›ç­”ã€‘
{candidates[i]}

ä»¥ä¸‹ã®åŸºæº–ã§è©•ä¾¡ã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
  "accuracy": <1-5>,
  "fluency": <1-5>,
  "relevance": <1-5>,
  "style": <1-5>,
  "total": <4-20>,
  "reasoning": "è©•ä¾¡ã®ç†ç”±"
}}"""

        try:
            response = judge_client.chat.completions.create(
                model="databricks-gpt-oss-120b",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å…¬å¹³ã§å®¢è¦³çš„ãªè©•ä¾¡è€…ã§ã™ã€‚"},
                    {"role": "user", "content": judge_prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            
            import json
            result = json.loads(response.choices[0].message.content)
            scores.append(result.get('total', 0))
        except:
            scores.append(0)
    
    return scores

# LLM-as-a-Judgeè©•ä¾¡ã®å®Ÿè¡Œã¨MLflowã¸ã®è¨˜éŒ²
with mlflow.start_run(run_name="llm-as-judge-evaluation") as run:
    print("ã€LLM-as-a-Judgeè©•ä¾¡ä¸­ã€‘")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    base_judge_scores = llm_as_judge_batch(
        eval_df['prompt'].tolist(),
        base_results['references'],
        base_results['predictions'],
        sample_size=10
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    finetuned_judge_scores = llm_as_judge_batch(
        eval_df['prompt'].tolist(),
        finetuned_results['references'],
        finetuned_results['predictions'],
        sample_size=10
    )
    
    # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    avg_base_judge = sum(base_judge_scores) / len(base_judge_scores)
    avg_finetuned_judge = sum(finetuned_judge_scores) / len(finetuned_judge_scores)
    
    # MLflowã«ãƒ­ã‚°
    mlflow.log_metric("llm_judge_base_avg", avg_base_judge)
    mlflow.log_metric("llm_judge_finetuned_avg", avg_finetuned_judge)
    mlflow.log_metric("llm_judge_improvement", avg_finetuned_judge - avg_base_judge)
    
    # è©³ç´°ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°
    judge_results_df = pd.DataFrame({
        'sample_id': range(len(base_judge_scores)),
        'base_score': base_judge_scores,
        'finetuned_score': finetuned_judge_scores
    })
    
    mlflow.log_table(judge_results_df, "llm_judge_scores.json")
    
    print(f"\nâœ… LLM-as-a-Judgeè©•ä¾¡å®Œäº†")
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å¹³å‡: {avg_base_judge:.2f}/20")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿å¹³å‡: {avg_finetuned_judge:.2f}/20")
    print(f"æ”¹å–„: +{(avg_finetuned_judge - avg_base_judge):.2f}ç‚¹")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 7: Model Registryã¸ã®ç™»éŒ²ã¨ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°

# COMMAND ----------

# DBTITLE 1,æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ¢ãƒ‡ãƒ«æ˜‡æ ¼
from mlflow.tracking import MlflowClient

client = MlflowClient()

# ç™»éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å–å¾—
model_name = "gemma-3-270m-finetuned-model-lora"

# æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—
latest_versions = client.get_latest_versions(model_name, stages=["None"])

if latest_versions:
    latest_version = latest_versions[0].version
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ã‚’ãƒã‚§ãƒƒã‚¯
    finetuned_metrics = finetuned_results['metrics']
    
    # åŸºæº–: BLEU > 20, BERTScore F1 > 0.7, ã”ã–ã‚‹ç‡ > 80%
    meets_criteria = (
        finetuned_metrics['bleu'] > 20 and
        finetuned_metrics['bertscore_f1'] > 0.7 and
        finetuned_metrics['gozaru_rate'] > 80
    )
    
    if meets_criteria:
        # Stagingã‚¹ãƒ†ãƒ¼ã‚¸ã«æ˜‡æ ¼
        client.transition_model_version_stage(
            name=model_name,
            version=latest_version,
            stage="Staging",
            archive_existing_versions=True
        )
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ {latest_version} ã‚’Stagingã«æ˜‡æ ¼ã—ã¾ã—ãŸ")
        print("\nã€æ€§èƒ½åŸºæº–ã€‘")
        print(f"  BLEU: {finetuned_metrics['bleu']:.2f} (åŸºæº–: > 20)")
        print(f"  BERTScore F1: {finetuned_metrics['bertscore_f1']:.4f} (åŸºæº–: > 0.7)")
        print(f"  ã”ã–ã‚‹ç‡: {finetuned_metrics['gozaru_rate']:.1f}% (åŸºæº–: > 80%)")
    else:
        print("âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒæ€§èƒ½åŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
        print("\nã€æ€§èƒ½åŸºæº–ã€‘")
        print(f"  BLEU: {finetuned_metrics['bleu']:.2f} (åŸºæº–: > 20) {'âœ…' if finetuned_metrics['bleu'] > 20 else 'âŒ'}")
        print(f"  BERTScore F1: {finetuned_metrics['bertscore_f1']:.4f} (åŸºæº–: > 0.7) {'âœ…' if finetuned_metrics['bertscore_f1'] > 0.7 else 'âŒ'}")
        print(f"  ã”ã–ã‚‹ç‡: {finetuned_metrics['gozaru_rate']:.1f}% (åŸºæº–: > 80%) {'âœ…' if finetuned_metrics['gozaru_rate'] > 80 else 'âŒ'}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 8: Databricks Model Servingã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆãƒ‡ãƒ¢ï¼‰

# COMMAND ----------

# DBTITLE 1,Model Serving Endpointã®ä½œæˆï¼ˆã‚³ãƒ¼ãƒ‰ä¾‹ï¼‰
# æ³¨æ„: å®Ÿéš›ã®å®Ÿè¡Œã«ã¯ã‚¯ãƒ©ã‚¹ã‚¿æ¨©é™ãŒå¿…è¦

deployment_code = """
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import ServedEntityInput, EndpointCoreConfigInput

w = WorkspaceClient()

# Model Serving Endpointã®ä½œæˆ
endpoint_name = "gemma-3-270m-finetuned-endpoint"

w.serving_endpoints.create(
    name=endpoint_name,
    config=EndpointCoreConfigInput(
        served_entities=[
            ServedEntityInput(
                entity_name="gemma-3-270m-finetuned-model-lora",
                entity_version="1",
                workload_size="Small",
                scale_to_zero_enabled=True
            )
        ]
    )
)

print(f"âœ… Serving Endpoint '{endpoint_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
"""

print("ã€Model Serving Endpointã®ä½œæˆã‚³ãƒ¼ãƒ‰ã€‘")
print(deployment_code)

print("\nğŸ“ æ³¨æ„:")
print("  - å®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯é©åˆ‡ãªæ¨©é™ãŒå¿…è¦ã§ã™")
print("  - Databricks Workspaceã®'Serving'ã‚¿ãƒ–ã‹ã‚‰æ‰‹å‹•ã§ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™")
print("  - ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã€REST APIã¾ãŸã¯SDKã§ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã›ã¾ã™")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 9: ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ

# COMMAND ----------

# DBTITLE 1,ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆã¨MLflowã¸ã®ä¿å­˜
import matplotlib.pyplot as plt
from datetime import datetime

# ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
report_content = f"""
{'='*80}
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ« ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ
{'='*80}

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
MLflowå®Ÿé¨“: {experiment_name}

ã€ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã€‘
ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_model_id}
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•: LoRA (r=16, alpha=32)
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: bbz662bbz/databricks-dolly-15k-ja-gozarinnemon
è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(eval_df)}

ã€è‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã€‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ¨™            â”‚ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« â”‚ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â”‚ æ”¹å–„ç‡ (%) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BLEU            â”‚ {base_results['metrics']['bleu']:>12.2f} â”‚ {finetuned_results['metrics']['bleu']:>16.2f} â”‚ {((finetuned_results['metrics']['bleu'] - base_results['metrics']['bleu']) / base_results['metrics']['bleu'] * 100):>10.2f} â”‚
â”‚ ROUGE-1         â”‚ {base_results['metrics']['rouge1']:>12.4f} â”‚ {finetuned_results['metrics']['rouge1']:>16.4f} â”‚ {((finetuned_results['metrics']['rouge1'] - base_results['metrics']['rouge1']) / base_results['metrics']['rouge1'] * 100):>10.2f} â”‚
â”‚ ROUGE-L         â”‚ {base_results['metrics']['rougeL']:>12.4f} â”‚ {finetuned_results['metrics']['rougeL']:>16.4f} â”‚ {((finetuned_results['metrics']['rougeL'] - base_results['metrics']['rougeL']) / base_results['metrics']['rougeL'] * 100):>10.2f} â”‚
â”‚ BERTScore (F1)  â”‚ {base_results['metrics']['bertscore_f1']:>12.4f} â”‚ {finetuned_results['metrics']['bertscore_f1']:>16.4f} â”‚ {((finetuned_results['metrics']['bertscore_f1'] - base_results['metrics']['bertscore_f1']) / base_results['metrics']['bertscore_f1'] * 100):>10.2f} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€ã‚¹ã‚¿ã‚¤ãƒ«è©•ä¾¡ã€‘
ã”ã–ã‚‹å£èª¿ä½¿ç”¨ç‡:
  ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {base_results['metrics']['gozaru_rate']:.1f}%
  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿: {finetuned_results['metrics']['gozaru_rate']:.1f}%
  æ”¹å–„: +{(finetuned_results['metrics']['gozaru_rate'] - base_results['metrics']['gozaru_rate']):.1f}ãƒã‚¤ãƒ³ãƒˆ

ã€LLM-as-a-Judgeè©•ä¾¡ã€‘
å¹³å‡ã‚¹ã‚³ã‚¢ï¼ˆ20ç‚¹æº€ç‚¹ï¼‰:
  ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {avg_base_judge:.2f}/20
  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿: {avg_finetuned_judge:.2f}/20
  æ”¹å–„: +{(avg_finetuned_judge - avg_base_judge):.2f}ç‚¹

ã€MLflow Runæƒ…å ±ã€‘
ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«Run ID: {base_results['run_id']}
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°Run ID: {finetuned_results['run_id']}

ã€çµè«–ã€‘
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã£ã¦å…¨ã¦ã®æŒ‡æ¨™ã§æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚
ç‰¹ã«ã€ã”ã–ã‚‹å£èª¿ã®ç²å¾—ã¨ã„ã†ç›®çš„ã¯é”æˆã•ã‚Œã¦ãŠã‚Šã€
ã‚¹ã‚¿ã‚¤ãƒ«ä¸€è²«æ€§ãŒ{(finetuned_results['metrics']['gozaru_rate'] - base_results['metrics']['gozaru_rate']):.1f}ãƒã‚¤ãƒ³ãƒˆå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã¯æ€§èƒ½åŸºæº–ã‚’æº€ãŸã—ã¦ãŠã‚Šã€Stagingã‚¹ãƒ†ãƒ¼ã‚¸ã«æ˜‡æ ¼å¯èƒ½ã§ã™ã€‚

{'='*80}
"""

print(report_content)

# ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
with open("/tmp/evaluation_report.txt", "w", encoding="utf-8") as f:
    f.write(report_content)

# MLflowã«ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ­ã‚°
with mlflow.start_run(run_id=finetuned_results['run_id']):
    mlflow.log_artifact("/tmp/evaluation_report.txt", "reports")
    
    # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚è¿½åŠ ãƒ­ã‚°
    mlflow.log_metric("overall_improvement_pct", 
                     (finetuned_results['metrics']['bleu'] - base_results['metrics']['bleu']) / base_results['metrics']['bleu'] * 100)

print("\nâœ… è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’MLflowã«ä¿å­˜ã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ Exercise 6ã®ã¾ã¨ã‚
# MAGIC
# MAGIC ã“ã®Exerciseã§å­¦ã‚“ã Databricks + MLflowã®æ´»ç”¨æ³•ï¼š
# MAGIC
# MAGIC ### MLflowã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†
# MAGIC 1. **è‡ªå‹•ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°**
# MAGIC    - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®è‡ªå‹•è¨˜éŒ²
# MAGIC    - è¤‡æ•°ã®å®Ÿé¨“ã‚’ä¸€å…ƒç®¡ç†
# MAGIC
# MAGIC 2. **MLflow Datasets**
# MAGIC    - è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
# MAGIC    - ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸ã®è¿½è·¡
# MAGIC
# MAGIC 3. **MLflow Evaluate**
# MAGIC    - æ¨™æº–åŒ–ã•ã‚ŒãŸLLMè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
# MAGIC    - ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æŒ‡æ¨™ã®è¿½åŠ 
# MAGIC
# MAGIC 4. **Model Registry**
# MAGIC    - ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
# MAGIC    - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆNone â†’ Staging â†’ Productionï¼‰
# MAGIC    - æ€§èƒ½åŸºæº–ã«åŸºã¥ãè‡ªå‹•æ˜‡æ ¼
# MAGIC
# MAGIC ### LLMè©•ä¾¡ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
# MAGIC 5. **è¤‡æ•°æŒ‡æ¨™ã®çµ„ã¿åˆã‚ã›**
# MAGIC    - BLEUã€ROUGEã€BERTScore
# MAGIC    - ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ï¼ˆã”ã–ã‚‹å£èª¿ä½¿ç”¨ç‡ï¼‰
# MAGIC    - LLM-as-a-Judge
# MAGIC
# MAGIC 6. **å¯è¦–åŒ–ã¨æ¯”è¼ƒ**
# MAGIC    - MLflow UIã§ã®å®Ÿé¨“æ¯”è¼ƒ
# MAGIC    - ã‚°ãƒ©ãƒ•ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®è‡ªå‹•ç”Ÿæˆ
# MAGIC
# MAGIC 7. **æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã¸ã®é“ç­‹**
# MAGIC    - Model Serving Endpointã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤
# MAGIC    - æ€§èƒ½åŸºæº–ã«åŸºã¥ãå“è³ªç®¡ç†

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
# MAGIC
# MAGIC 1. **ç¶™ç¶šçš„ãªæ”¹å–„**
# MAGIC    - MLflowã§è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’æ¯”è¼ƒ
# MAGIC    - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆHyperopt + MLflowï¼‰
# MAGIC
# MAGIC 2. **A/Bãƒ†ã‚¹ãƒˆ**
# MAGIC    - Model Servingã§è¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¸¦è¡Œç¨¼åƒ
# MAGIC    - ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ã«ã‚ˆã‚‹æ€§èƒ½æ¯”è¼ƒ
# MAGIC
# MAGIC 3. **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**
# MAGIC    - Lakehouse Monitoringã§ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º
# MAGIC    - æœ¬ç•ªç’°å¢ƒã§ã®å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
# MAGIC
# MAGIC 4. **CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**
# MAGIC    - Databricks Workflowsã§è©•ä¾¡ã‚’è‡ªå‹•åŒ–
# MAGIC    - GitHub Actionsã¨ã®çµ±åˆ
