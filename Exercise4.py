# Databricks notebook source
# MAGIC %md
# MAGIC # Exercise 4: HuggingFaceãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼ˆGemma 3 270Mï¼‰
# MAGIC
# MAGIC ## ç›®çš„
# MAGIC - HuggingFace Hubã‹ã‚‰ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶
# MAGIC - `apply_chat_template`ã‚’ä½¿ã£ãŸæ­£ã—ã„ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®å®Ÿè£…
# MAGIC - Foundation Model APIä»¥å¤–ã®é¸æŠè‚¢ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼‰ã‚’ç†è§£ã™ã‚‹
# MAGIC - è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆ270Mï¼‰ã®å®Ÿç”¨æ€§ã‚’è©•ä¾¡ã™ã‚‹
# MAGIC
# MAGIC ## Gemma 3 270Mã«ã¤ã„ã¦
# MAGIC - **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: 270Mï¼ˆ2.7å„„ï¼‰
# MAGIC - **ç‰¹å¾´**: Gemma 3ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®æœ€å°ãƒ¢ãƒ‡ãƒ«ã€ãƒ¢ãƒã‚¤ãƒ«ãƒ»ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹å‘ã‘
# MAGIC - **ç”¨é€”**: è³ªå•å¿œç­”ã€è¦ç´„ã€åˆ†é¡ã€è»½é‡æ¨è«–ã‚¿ã‚¹ã‚¯
# MAGIC - **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„540MBã€INT8ã§ç´„270MB

# COMMAND ----------

# MAGIC %md
# MAGIC ## ç’°å¢ƒç¢ºèªã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

# COMMAND ----------

# DBTITLE 1,å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# MAGIC %pip install --upgrade transformers accelerate sentencepiece
# MAGIC
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,GPUç¢ºèª
import torch

print("ã€GPUãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã€‘")
if torch.cuda.is_available():
    print(f"âœ… GPUåˆ©ç”¨å¯èƒ½")
    print(f"GPUæ•°: {torch.cuda.device_count()}")
    print(f"ç¾åœ¨ã®GPU: {torch.cuda.current_device()}")
    print(f"GPUå: {torch.cuda.get_device_name(0)}")
    print(f"CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
    
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ç·ãƒ¡ãƒ¢ãƒª: {total_memory:.2f} GB")
else:
    print("âš ï¸ GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚")

# COMMAND ----------

# DBTITLE 1,ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
import transformers
import torch

print("ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘")
print(f"transformers: {transformers.__version__}")
print(f"torch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 1: ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰

# COMMAND ----------

import os

# HuggingFace Hubã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
os.environ["HF_TOKEN"] = "<your_huggingface_access_token>"

# COMMAND ----------

# DBTITLE 1,ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "google/gemma-3-270m-it"

print(f"ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã€‘")
print(f"Model ID: {model_id}")
print("åˆå›å®Ÿè¡Œæ™‚ã¯1-2åˆ†ã‹ã‹ã‚Šã¾ã™...\n")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")
print(f"ãƒ‡ãƒ¼ã‚¿å‹: {model.dtype}")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}")

if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    print(f"GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {allocated:.2f} GB")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2: apply_chat_templateã‚’ä½¿ã£ãŸæ­£ã—ã„ä½¿ã„æ–¹
# MAGIC
# MAGIC Gemma 3ã¯ãƒãƒ£ãƒƒãƒˆç”¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å†…è”µã—ã¦ãŠã‚Šã€
# MAGIC `apply_chat_template`ã‚’ä½¿ã†ã“ã¨ã§é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒè‡ªå‹•é©ç”¨ã•ã‚Œã¾ã™ã€‚

# COMMAND ----------

# DBTITLE 1,åŸºæœ¬çš„ãªä¼šè©±ï¼ˆä½ãƒ¬ãƒ™ãƒ«APIï¼‰
def chat_with_model(messages: list, max_new_tokens: int = 100, temperature: float = 0.7) -> str:
    """
    apply_chat_templateã‚’ä½¿ã£ãŸä¼šè©±ç”Ÿæˆ
    
    Args:
        messages: OpenAIå½¢å¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
        max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        temperature: ç”Ÿæˆã®ãƒ©ãƒ³ãƒ€ãƒ æ€§
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,  # ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True if temperature > 0 else False,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤ã„ã¦ã€ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return response

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
messages = [
    {"role": "user", "content": "æ—¥æœ¬ã®æœ€ã‚‚åŒ—ã«ã‚ã‚‹éƒ½é“åºœçœŒã¯ï¼Ÿ"}
]

print("ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€‘")
print(messages[0]["content"])
print("\nã€ãƒ¢ãƒ‡ãƒ«ã€‘")
response = chat_with_model(messages, max_new_tokens=150, temperature=0.7)
print(response)

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ’¡ apply_chat_templateã®ãƒã‚¤ãƒ³ãƒˆ
# MAGIC
# MAGIC - `add_generation_prompt=True`: ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’ä¿ƒã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•è¿½åŠ 
# MAGIC - `tokenize=True, return_tensors="pt"`: ç›´æ¥PyTorchãƒ†ãƒ³ã‚½ãƒ«ã§è¿”ã™
# MAGIC - `return_dict=True`: è¾æ›¸å½¢å¼ã§è¿”ã™ï¼ˆ`model.generate()`ã«ç›´æ¥æ¸¡ã›ã‚‹ï¼‰
# MAGIC - å‡ºåŠ›ã‹ã‚‰å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å¤–: `outputs[0][inputs["input_ids"].shape[-1]:]`

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3: Pipelineã‚’ä½¿ã£ãŸã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•

# COMMAND ----------

# DBTITLE 1,Pipeline APIã«ã‚ˆã‚‹ä¼šè©±
from transformers import pipeline

# ãƒãƒ£ãƒƒãƒˆç”¨ã®pipelineã‚’ä½œæˆ
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7
)

print("ã€Pipeline APIã‚’ä½¿ç”¨ã—ãŸä¼šè©±ã€‘\n")

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã§ä¼šè©±
messages = [
    {"role": "user", "content": "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
]

result = pipe(messages)

print("ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€‘")
print(messages[0]["content"])
print("\nã€ãƒ¢ãƒ‡ãƒ«ã€‘")
# Pipelineã¯ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’è¿”ã™ã®ã§ã€æœ€å¾Œã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’æŠ½å‡º
print(result[0]["generated_text"][-1]["content"])

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 4: ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±
# MAGIC
# MAGIC ä¼šè©±å±¥æ­´ã‚’ä¿æŒã—ãŸå¯¾è©±ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

# COMMAND ----------

# DBTITLE 1,ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã®å®Ÿè£…
def multi_turn_chat(conversation_history: list, user_message: str) -> tuple:
    """
    ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±
    
    Args:
        conversation_history: ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        user_message: æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
    Returns:
        (ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”, æ›´æ–°ã•ã‚ŒãŸä¼šè©±å±¥æ­´)
    """
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    conversation_history.append({"role": "user", "content": user_message})
    
    # ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’ç”Ÿæˆ
    response = chat_with_model(conversation_history, max_new_tokens=150)
    
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
    conversation_history.append({"role": "assistant", "content": response})
    
    return response, conversation_history

# ä¼šè©±ã®é–‹å§‹
conversation = []

print("=" * 60)
print("ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã®ãƒ‡ãƒ¢")
print("=" * 60)

# ã‚¿ãƒ¼ãƒ³1
print("\nã€ã‚¿ãƒ¼ãƒ³1ã€‘")
user_msg_1 = "æ±ºå®šæœ¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„"
print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg_1}")
assistant_msg_1, conversation = multi_turn_chat(conversation, user_msg_1)
print(f"ãƒ¢ãƒ‡ãƒ«: {assistant_msg_1}")

# ã‚¿ãƒ¼ãƒ³2ï¼ˆæ–‡è„ˆã‚’å‚ç…§ï¼‰
print("\nã€ã‚¿ãƒ¼ãƒ³2ã€‘")
user_msg_2 = "ãã‚Œã§ã¯ã€æ¬ ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ"
print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg_2}")
assistant_msg_2, conversation = multi_turn_chat(conversation, user_msg_2)
print(f"ãƒ¢ãƒ‡ãƒ«: {assistant_msg_2}")

# ã‚¿ãƒ¼ãƒ³3
print("\nã€ã‚¿ãƒ¼ãƒ³3ã€‘")
user_msg_3 = "ãã®æ¬ ç‚¹ã‚’å…‹æœã™ã‚‹æ–¹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg_3}")
assistant_msg_3, conversation = multi_turn_chat(conversation, user_msg_3)
print(f"ãƒ¢ãƒ‡ãƒ«: {assistant_msg_3}")

print("\n" + "=" * 60)

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ” æ–‡è„ˆã®ä¿æŒã‚’ç¢ºèª
# MAGIC
# MAGIC - ã‚¿ãƒ¼ãƒ³2ã®ã€Œãã‚Œã€ãŒã€Œæ±ºå®šæœ¨ã€ã‚’æŒ‡ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç†è§£
# MAGIC - ã‚¿ãƒ¼ãƒ³3ã®ã€Œãã®æ¬ ç‚¹ã€ãŒå‰ã®å¿œç­”ã‚’å‚ç…§
# MAGIC - ã“ã‚ŒãŒapply_chat_templateã«ã‚ˆã‚‹æ­£ã—ã„ä¼šè©±ç®¡ç†

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 5: æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ã®è©•ä¾¡

# COMMAND ----------

# DBTITLE 1,ã‚¿ã‚¹ã‚¯1: è³ªå•å¿œç­”
print("=" * 60)
print("ã‚¿ã‚¹ã‚¯1: è³ªå•å¿œç­”")
print("=" * 60 + "\n")

qa_messages = [
    {"role": "user", "content": "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¸»è¦ãª3ã¤ã®æ§‹æˆè¦ç´ ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"}
]

qa_response = chat_with_model(qa_messages, max_new_tokens=200, temperature=0.5)

print("ã€è³ªå•ã€‘")
print(qa_messages[0]["content"])
print("\nã€å›ç­”ã€‘")
print(qa_response)

# COMMAND ----------

# DBTITLE 1,ã‚¿ã‚¹ã‚¯2: ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„
print("=" * 60)
print("ã‚¿ã‚¹ã‚¯2: ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„")
print("=" * 60 + "\n")

long_text = """
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯ã€è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨å‘¼ã°ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ã„ã¦ãŠã‚Šã€
Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦æ–‡è„ˆã‚’ç†è§£ã—ã¾ã™ã€‚
GPTã€BERTã€LLaMAã€Gemmaãªã©ãŒä»£è¡¨çš„ãªLLMã§ã™ã€‚
è¿‘å¹´ã§ã¯ã€æ•°åƒå„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚‚ç™»å ´ã—ã€
è³ªå•å¿œç­”ã€æ–‡ç« ç”Ÿæˆã€ç¿»è¨³ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãªã©ã€å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã§äººé–“ãƒ¬ãƒ™ãƒ«ã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¦ã„ã¾ã™ã€‚
"""

summary_messages = [
    {"role": "user", "content": f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’1-2æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n{long_text}"}
]

summary = chat_with_model(summary_messages, max_new_tokens=100, temperature=0.3)

print("ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã€‘")
print(long_text)
print("\nã€è¦ç´„ã€‘")
print(summary)

# COMMAND ----------

# DBTITLE 1,ã‚¿ã‚¹ã‚¯3: æ„Ÿæƒ…åˆ†é¡
print("=" * 60)
print("ã‚¿ã‚¹ã‚¯3: æ„Ÿæƒ…åˆ†æ")
print("=" * 60 + "\n")

reviews = [
    "ã“ã®è£½å“ã¯ç´ æ™´ã‚‰ã—ã„ï¼æœŸå¾…ä»¥ä¸Šã®æ€§èƒ½ã§ã™ã€‚",
    "ä½¿ã„ã«ããã¦æœ€æ‚ªã§ã—ãŸã€‚ã™ãã«å£Šã‚Œã¾ã—ãŸã€‚",
    "ã¾ã‚ã¾ã‚ã§ã™ã€‚ç‰¹ã«è‰¯ãã‚‚æ‚ªãã‚‚ãªã„ã€‚"
]

for i, review in enumerate(reviews, 1):
    messages = [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚å˜èªã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nãƒ¬ãƒ“ãƒ¥ãƒ¼: {review}"
        }
    ]
    
    sentiment = chat_with_model(messages, max_new_tokens=10, temperature=0.1)
    
    print(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼{i}: {review}")
    print(f"æ„Ÿæƒ…: {sentiment}\n")

# COMMAND ----------

# DBTITLE 1,ã‚¿ã‚¹ã‚¯4: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½¿ç”¨
print("=" * 60)
print("ã‚¿ã‚¹ã‚¯4: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹æŒ¯ã‚‹èˆã„åˆ¶å¾¡")
print("=" * 60 + "\n")

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
messages_with_system = [
    {
        "role": "system",
        "content": "ã‚ãªãŸã¯è¦ªåˆ‡ãªå…ˆç”Ÿã§ã™ã€‚å°‚é–€ç”¨èªã‚’é¿ã‘ã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    },
    {
        "role": "user",
        "content": "éå­¦ç¿’ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    }
]

response_with_system = chat_with_model(messages_with_system, max_new_tokens=200, temperature=0.7)

print("ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ãã€‘")
print(response_with_system)

print("\n" + "-"*60 + "\n")

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—
messages_without_system = [
    {
        "role": "user",
        "content": "éå­¦ç¿’ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    }
]

response_without_system = chat_with_model(messages_without_system, max_new_tokens=200, temperature=0.7)

print("ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—ã€‘")
print(response_without_system)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 6: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

# COMMAND ----------

# DBTITLE 1,æ¨è«–é€Ÿåº¦ã®æ¸¬å®š
import time

def measure_inference_speed(messages: list, num_runs: int = 5) -> dict:
    """æ¨è«–é€Ÿåº¦ã‚’æ¸¬å®š"""
    times = []
    
    for _ in range(num_runs):
        start_time = time.time()
        _ = chat_with_model(messages, max_new_tokens=50, temperature=0.7)
        end_time = time.time()
        times.append(end_time - start_time)
    
    return {
        "å¹³å‡æ™‚é–“": sum(times) / len(times),
        "æœ€å°æ™‚é–“": min(times),
        "æœ€å¤§æ™‚é–“": max(times)
    }

test_messages = [
    {"role": "user", "content": "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"}
]

print("ã€æ¨è«–é€Ÿåº¦æ¸¬å®šã€‘")
print("5å›ã®å®Ÿè¡Œã§æ¸¬å®šä¸­...\n")
stats = measure_inference_speed(test_messages, num_runs=5)

for key, value in stats.items():
    print(f"{key}: {value:.4f}ç§’")

avg_tokens_per_sec = 50 / stats["å¹³å‡æ™‚é–“"]
print(f"\næ¨å®šã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {avg_tokens_per_sec:.2f} tokens/ç§’")

# COMMAND ----------

# DBTITLE 1,ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª
if torch.cuda.is_available():
    print("ã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€‘")
    
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    reserved = torch.cuda.memory_reserved(0) / 1024**3
    max_allocated = torch.cuda.max_memory_allocated(0) / 1024**3
    
    print(f"ç¾åœ¨ã®å‰²ã‚Šå½“ã¦: {allocated:.2f} GB")
    print(f"äºˆç´„æ¸ˆã¿: {reserved:.2f} GB")
    print(f"æœ€å¤§å‰²ã‚Šå½“ã¦: {max_allocated:.2f} GB")
    
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    utilization = (allocated / total_memory) * 100
    print(f"ãƒ¡ãƒ¢ãƒªåˆ©ç”¨ç‡: {utilization:.2f}%")
else:
    print("âš ï¸ CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 7: ãƒãƒƒãƒæ¨è«–

# COMMAND ----------

# DBTITLE 1,ãƒãƒƒãƒæ¨è«–ã®å®Ÿè£…
def batch_chat(messages_list: list, max_new_tokens: int = 100) -> list:
    """
    è¤‡æ•°ã®ä¼šè©±ã‚’ãƒãƒƒãƒå‡¦ç†
    
    Args:
        messages_list: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    
    Returns:
        å¿œç­”ã®ãƒªã‚¹ãƒˆ
    """
    # å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
    batch_inputs = []
    for messages in messages_list:
        formatted = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=False  # ã¾ãšæ–‡å­—åˆ—ã¨ã—ã¦å–å¾—
        )
        batch_inputs.append(formatted)
    
    # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer(
        batch_inputs,
        return_tensors="pt",
        padding=True,
        truncation=True
    ).to(model.device)
    
    # ãƒãƒƒãƒç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # å„å‡ºåŠ›ã‹ã‚‰å…¥åŠ›éƒ¨åˆ†ã‚’é™¤ã„ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
    responses = []
    for i, output in enumerate(outputs):
        input_length = inputs["input_ids"][i].shape[0]
        generated_ids = output[input_length:]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True)
        responses.append(response)
    
    return responses

# ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ
batch_messages = [
    [{"role": "user", "content": "æ©Ÿæ¢°å­¦ç¿’ã‚’ä¸€è¨€ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚"}],
    [{"role": "user", "content": "Pythonã®ç‰¹å¾´ã‚’1ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚"}],
    [{"role": "user", "content": "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã«å¿…è¦ãªã‚¹ã‚­ãƒ«ã¯ï¼Ÿ"}],
]

print("ã€ãƒãƒƒãƒæ¨è«–ã€‘")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {len(batch_messages)}\n")

start_time = time.time()
batch_responses = batch_chat(batch_messages, max_new_tokens=80)
batch_time = time.time() - start_time

for i, (messages, response) in enumerate(zip(batch_messages, batch_responses), 1):
    print(f"ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ{i}ã€‘ {messages[0]['content']}")
    print(f"ã€å¿œç­”ã€‘ {response}")
    print("-" * 60)

print(f"\nãƒãƒƒãƒå‡¦ç†æ™‚é–“: {batch_time:.4f}ç§’")
print(f"1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚ãŸã‚Š: {batch_time/len(batch_messages):.4f}ç§’")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ Exercise 4ã®ã¾ã¨ã‚
# MAGIC
# MAGIC ã“ã®Exerciseã§å­¦ã‚“ã ã“ã¨ï¼š
# MAGIC
# MAGIC ### æ­£ã—ã„Gemma 3ã®ä½¿ç”¨æ–¹æ³•
# MAGIC 1. **apply_chat_templateã®ä½¿ç”¨**
# MAGIC    - ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒãƒ£ãƒƒãƒˆå½¢å¼ã‚’è‡ªå‹•é©ç”¨
# MAGIC    - `add_generation_prompt=True`ã§å¿œç­”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
# MAGIC    - OpenAIå½¢å¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§çµ±ä¸€çš„ã«æ‰±ãˆã‚‹
# MAGIC
# MAGIC 2. **2ã¤ã®API**
# MAGIC    - ä½ãƒ¬ãƒ™ãƒ«: `tokenizer.apply_chat_template()` + `model.generate()`
# MAGIC    - é«˜ãƒ¬ãƒ™ãƒ«: `pipeline("text-generation")`
# MAGIC
# MAGIC 3. **ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±**
# MAGIC    - ä¼šè©±å±¥æ­´ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã§ç®¡ç†
# MAGIC    - æ–‡è„ˆã‚’ä¿æŒã—ãŸè‡ªç„¶ãªå¯¾è©±
# MAGIC
# MAGIC 4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†**
# MAGIC    - GPU/CPUã®è‡ªå‹•é¸æŠ
# MAGIC    - ãƒãƒƒãƒæ¨è«–ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
# MAGIC    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Foundation Model API vs ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«
# MAGIC
# MAGIC | é …ç›® | Gemma 3 270M (ãƒ­ãƒ¼ã‚«ãƒ«) | Foundation Model API |
# MAGIC |------|------------------------|----------------------|
# MAGIC | **å®Ÿè£…ã®ç°¡å˜ã•** | apply_chat_templateå¿…è¦ | OpenAIäº’æ›ã§çµ±ä¸€ |
# MAGIC | **ã‚³ã‚¹ãƒˆ** | GPUæ™‚é–“ã®ã¿ï¼ˆå›ºå®šï¼‰ | ãƒˆãƒ¼ã‚¯ãƒ³èª²é‡‘ï¼ˆå¤‰å‹•ï¼‰ |
# MAGIC | **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼** | ä½ã„ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸è¦ï¼‰ | ã‚„ã‚„é«˜ã„ï¼ˆAPIå‘¼ã³å‡ºã—ï¼‰ |
# MAGIC | **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º** | 270Mï¼ˆè»½é‡ï¼‰ | 70Bã€œï¼ˆé«˜æ€§èƒ½ï¼‰ |
# MAGIC | **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º** | ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ |
# MAGIC | **ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹** | è‡ªå·±ç®¡ç† | ãƒãƒãƒ¼ã‚¸ãƒ‰ |

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¡ ä½¿ã„åˆ†ã‘ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
# MAGIC
# MAGIC ### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆGemma 3ãªã©ï¼‰ã‚’ä½¿ã†ã¹ãå ´åˆ
# MAGIC - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ãŒå¿…è¦ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ < 100msï¼‰
# MAGIC - å¤§é‡ã®ç¶™ç¶šçš„ãªæ¨è«–ï¼ˆã‚³ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§ï¼‰
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦
# MAGIC - ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤
# MAGIC - å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãŒå¿…è¦
# MAGIC
# MAGIC ### Foundation Model APIã‚’ä½¿ã†ã¹ãå ´åˆ
# MAGIC - æœ€é«˜å“è³ªã®å‡ºåŠ›ãŒå¿…è¦ï¼ˆ70B+ãƒ¢ãƒ‡ãƒ«ï¼‰
# MAGIC - ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã®è¿…é€Ÿãªé–‹ç™º
# MAGIC - ä¸å®šæœŸãªãƒãƒƒãƒå‡¦ç†
# MAGIC - ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†ã‚’é¿ã‘ãŸã„
# MAGIC - çµ±ä¸€ã•ã‚ŒãŸOpenAIäº’æ›APIãŒå¿…è¦

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ ç·æ‹¬
# MAGIC
# MAGIC Exercise 4ã§ã¯ã€**apply_chat_templateã‚’ä½¿ã£ãŸæ­£ã—ã„ãƒãƒ£ãƒƒãƒˆå½¢å¼**ã§
# MAGIC ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ã†æ–¹æ³•ã‚’å­¦ã³ã¾ã—ãŸã€‚
# MAGIC
# MAGIC ã“ã‚Œã«ã‚ˆã‚Šï¼š
# MAGIC - ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ„è­˜ã›ãšã«ä¼šè©±ã‚’å®Ÿè£…
# MAGIC - OpenAIå½¢å¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§çµ±ä¸€çš„ã«æ‰±ãˆã‚‹
# MAGIC - ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚„æ–‡è„ˆä¿æŒãŒå®¹æ˜“
# MAGIC - Foundation Model APIã¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½¿ã„åˆ†ã‘ã‚’ç†è§£
# MAGIC
# MAGIC æ¬¡å›ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¬›ç¾©ã§ã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’
# MAGIC **è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã¨é€£æºã•ã›ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ **ã«çµ±åˆã—ã¾ã™ï¼
