# Databricks notebook source
# MAGIC %md
# MAGIC # Exercise 5: Gemma 3 270Mã®LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# MAGIC
# MAGIC ## ç›®çš„
# MAGIC - LoRAï¼ˆLow-Rank Adaptationï¼‰ã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å­¦ã¶
# MAGIC - æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®Instruction Tuningã‚’å®Ÿè·µã™ã‚‹
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰å¾Œã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹
# MAGIC - Databricksç’°å¢ƒã§ã®GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½“é¨“ã™ã‚‹
# MAGIC
# MAGIC ## ä½¿ç”¨ã™ã‚‹ã‚‚ã®
# MAGIC - **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: google/gemma-3-270m-it
# MAGIC - **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: bbz662bbz/databricks-dolly-15k-ja-gozarinnemon
# MAGIC   - Databricks Dolly 15kã®æ—¥æœ¬èªè¨³ç‰ˆã§ã€å›ç­”ã®èªå°¾ãŒã€Œã”ã–ã‚‹ã€å£èª¿
# MAGIC - **æ‰‹æ³•**: LoRAï¼ˆParameter-Efficient Fine-Tuningï¼‰
# MAGIC - **ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: Hugging Face Transformers, PEFT, TRL

# COMMAND ----------

# MAGIC %md
# MAGIC ## âš ï¸ é‡è¦: GPUç’°å¢ƒã®ç¢ºèª
# MAGIC
# MAGIC ã“ã®Notebookã¯**GPUå¿…é ˆ**ã§ã™ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š
# MAGIC
# MAGIC 1. **Serverless GPU**ï¼ˆæ¨å¥¨ï¼‰
# MAGIC    - Notebookã®Connect â†’ Serverless GPU
# MAGIC    - A10ã¾ãŸã¯H100ã‚’é¸æŠ
# MAGIC
# MAGIC 2. **Single Node GPU Cluster**
# MAGIC    - g4dn.xlargeä»¥ä¸Šã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
# MAGIC    - Databricks Runtime 14.3 MLä»¥é™

# COMMAND ----------

# DBTITLE 1,GPUç¢ºèª
import torch

print("ã€GPUç’°å¢ƒç¢ºèªã€‘")
if torch.cuda.is_available():
    print(f"âœ… GPUåˆ©ç”¨å¯èƒ½")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"ç·ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
else:
    print("âŒ GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    print("ã“ã®Notebookã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯GPUç’°å¢ƒãŒå¿…è¦ã§ã™")
    raise RuntimeError("GPU not available")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

# COMMAND ----------

# DBTITLE 1,å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# MAGIC %pip install --upgrade transformers datasets accelerate peft trl bitsandbytes sentencepiece
# MAGIC
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
import transformers
import datasets
import peft
import trl
import torch

print("ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‘")
print(f"transformers: {transformers.__version__}")
print(f"datasets: {datasets.__version__}")
print(f"peft: {peft.__version__}")
print(f"trl: {trl.__version__}")
print(f"torch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

# COMMAND ----------

# DBTITLE 1,ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
from datasets import load_dataset

dataset_name = "bbz662bbz/databricks-dolly-15k-ja-gozarinnemon"

print(f"ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã€‘")
print(f"Dataset: {dataset_name}\n")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
dataset = load_dataset(dataset_name, split="train")

print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰å®Œäº†")
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset):,}")
print(f"\nã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã€‘")
print(dataset)

# COMMAND ----------

# DBTITLE 1,ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
import random

# ãƒ©ãƒ³ãƒ€ãƒ ã«3ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
print("ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ã€‘\n")

for i in range(3):
    idx = random.randint(0, len(dataset) - 1)
    sample = dataset[idx]
    
    print(f"ã‚µãƒ³ãƒ—ãƒ« {i+1}:")
    print(f"ã‚«ãƒ†ã‚´ãƒªãƒ¼: {sample.get('category', 'N/A')}")
    print(f"æŒ‡ç¤º: {sample['instruction']}")
    
    if sample.get('input'):
        print(f"å…¥åŠ›: {sample['input']}")
    
    print(f"å‡ºåŠ›: {sample['output']}")
    print("-" * 80 + "\n")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›

# COMMAND ----------

# DBTITLE 1,ãƒãƒ£ãƒƒãƒˆå½¢å¼ã¸ã®å¤‰æ›é–¢æ•°
def format_to_chat(example):
    """
    Dollyå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
    
    Dollyå½¢å¼:
    - instruction: æŒ‡ç¤º
    - input: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    - output: æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›
    
    ãƒãƒ£ãƒƒãƒˆå½¢å¼:
    - messages: [{"role": "user", "content": ...}, {"role": "assistant", "content": ...}]
    """
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹ç¯‰
    if example.get('input') and example['input'].strip():
        # inputãŒã‚ã‚‹å ´åˆã¯ã€instruction + input
        user_content = f"{example['instruction']}\n\n{example['input']}"
    else:
        # inputãŒãªã„å ´åˆã¯ã€instructionã®ã¿
        user_content = example['instruction']
    
    # ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
    messages = [
        {"role": "user", "content": user_content},
        {"role": "assistant", "content": example['output']}
    ]
    
    return {"messages": messages}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’å¤‰æ›
print("ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›ä¸­ã€‘")
dataset = dataset.map(
    format_to_chat,
    remove_columns=dataset.column_names
)

print("âœ… å¤‰æ›å®Œäº†\n")
print("ã€å¤‰æ›å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ã€‘")
print(dataset[0]['messages'])

# COMMAND ----------

# DBTITLE 1,Train/Testã‚¹ãƒ—ãƒªãƒƒãƒˆ
# 80%ã‚’è¨“ç·´ç”¨ã€20%ã‚’ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²
dataset = dataset.train_test_split(test_size=0.2, seed=42)

print("ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²ã€‘")
print(f"è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset['train']):,}")
print(f"ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset['test']):,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰

# COMMAND ----------

import os

# HuggingFace Hubã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
os.environ["HF_TOKEN"] = "<your_huggingface_access_token>"

# COMMAND ----------

# DBTITLE 1,ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

model_id = "google/gemma-3-270m-it"

print(f"ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã€‘")
print(f"Model ID: {model_id}\n")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šï¼ˆLoRAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ï¼‰
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 4-bité‡å­åŒ–è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆ4-bité‡å­åŒ–ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
allocated = torch.cuda.memory_allocated(0) / 1024**3
print(f"GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {allocated:.2f} GB")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ’¡ 4-bité‡å­åŒ–ã®åŠ¹æœ
# MAGIC
# MAGIC - **é€šå¸¸ï¼ˆFP16ï¼‰**: ç´„540MB
# MAGIC - **4-bité‡å­åŒ–**: ç´„135MB
# MAGIC - **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡**: ç´„75%
# MAGIC
# MAGIC ã“ã‚Œã«ã‚ˆã‚Šã€å°è¦æ¨¡GPUã§ã‚‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 4: LoRAè¨­å®š

# COMMAND ----------

# DBTITLE 1,LoRAè¨­å®šã®å®šç¾©
from peft import LoraConfig, TaskType, prepare_model_for_kbit_training

# ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«æº–å‚™
model = prepare_model_for_kbit_training(model)

# LoRAè¨­å®š
lora_config = LoraConfig(
    r=16,                                    # LoRAã®ãƒ©ãƒ³ã‚¯ï¼ˆä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®æ¬¡å…ƒï¼‰
    lora_alpha=32,                          # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
    target_modules=["q_proj", "v_proj"],   # LoRAã‚’é©ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    lora_dropout=0.05,                      # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
    bias="none",                            # ãƒã‚¤ã‚¢ã‚¹ã®æ‰±ã„
    task_type=TaskType.CAUSAL_LM           # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
)

print("ã€LoRAè¨­å®šã€‘")
print(f"ãƒ©ãƒ³ã‚¯ (r): {lora_config.r}")
print(f"Alpha: {lora_config.lora_alpha}")
print(f"å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {lora_config.target_modules}")
print(f"ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ: {lora_config.lora_dropout}")
print(f"ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {lora_config.task_type}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ’¡ LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜
# MAGIC
# MAGIC | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | èª¬æ˜ | æ¨å¥¨å€¤ |
# MAGIC |----------|------|--------|
# MAGIC | **r** | LoRAãƒ©ãƒ³ã‚¯ï¼ˆä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®æ¬¡å…ƒï¼‰ | 8-32ï¼ˆå°ã•ã„ã»ã©åŠ¹ç‡çš„ã€å¤§ãã„ã»ã©è¡¨ç¾åŠ›ãŒé«˜ã„ï¼‰ |
# MAGIC | **lora_alpha** | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ | é€šå¸¸ã¯`r`ã®2å€ |
# MAGIC | **target_modules** | LoRAã‚’é©ç”¨ã™ã‚‹å±¤ | Attentionå±¤ï¼ˆq_proj, v_projï¼‰ãŒä¸€èˆ¬çš„ |
# MAGIC | **lora_dropout** | éå­¦ç¿’é˜²æ­¢ | 0.05-0.1 |
# MAGIC
# MAGIC **è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‰Šæ¸›**:
# MAGIC - å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: 270Mï¼ˆ100%ï¼‰
# MAGIC - LoRAï¼ˆr=16ï¼‰: ç´„0.5-1Mï¼ˆ0.2-0.4%ï¼‰

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 5: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®æ€§èƒ½è©•ä¾¡

# COMMAND ----------

# DBTITLE 1,ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®ãƒ†ã‚¹ãƒˆ
def test_model(model, tokenizer, test_prompts):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹é–¢æ•°"""
    print("ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®æ€§èƒ½ã€‘\n")
    
    for i, prompt in enumerate(test_prompts, 1):
        messages = [{"role": "user", "content": prompt}]
        
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt"
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        print(f"è³ªå• {i}: {prompt}")
        print(f"å›ç­”: {response}")
        print("-" * 80 + "\n")

# ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
test_prompts = [
    "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
    "Pythonã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
]

test_model(model, tokenizer, test_prompts)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 6: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š

# COMMAND ----------

# DBTITLE 1,ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®è¨­å®š
from transformers import TrainingArguments

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
output_dir = "/dbfs/tmp/gemma-3-270m-finetuned-gozaru"

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,                    # ã‚¨ãƒãƒƒã‚¯æ•°
    per_device_train_batch_size=4,        # ãƒãƒƒãƒã‚µã‚¤ã‚º
    gradient_accumulation_steps=4,         # å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º=16ï¼‰
    learning_rate=2e-4,                    # å­¦ç¿’ç‡
    lr_scheduler_type="cosine",            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    warmup_ratio=0.1,                      # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¯”ç‡
    logging_steps=10,                      # ãƒ­ã‚°å‡ºåŠ›é »åº¦
    save_strategy="epoch",                 # ä¿å­˜æˆ¦ç•¥
    eval_strategy="epoch",                 # è©•ä¾¡æˆ¦ç•¥
    bf16=True,                             # BFloat16ç²¾åº¦
    gradient_checkpointing=True,           # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
    remove_unused_columns=False,           # æœªä½¿ç”¨åˆ—ã‚’å‰Šé™¤ã—ãªã„
    report_to="none",                      # ãƒ¬ãƒãƒ¼ãƒˆå…ˆï¼ˆä»Šå›ã¯ãªã—ï¼‰
)

print("ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã€‘")
print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}")
print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}")
print(f"å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—: {training_args.gradient_accumulation_steps}")
print(f"å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"å­¦ç¿’ç‡: {training_args.learning_rate}")
print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ’¡ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´
# MAGIC
# MAGIC **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ**:
# MAGIC 1. `per_device_train_batch_size`ã‚’2ã«æ¸›ã‚‰ã™
# MAGIC 2. `gradient_accumulation_steps`ã‚’8ã«å¢—ã‚„ã™ï¼ˆå®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºç¶­æŒï¼‰
# MAGIC 3. `gradient_checkpointing=True`ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆæ—¢ã«æœ‰åŠ¹ï¼‰
# MAGIC
# MAGIC **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’çŸ­ç¸®ã—ãŸã„å ´åˆ**:
# MAGIC 1. `num_train_epochs`ã‚’1-2ã«æ¸›ã‚‰ã™
# MAGIC 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¬¡ã®ã‚»ãƒ«ã§å®Ÿè£…ï¼‰

# COMMAND ----------

# DBTITLE 1,ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
# é«˜é€Ÿãƒ†ã‚¹ãƒˆç”¨ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€éƒ¨ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
USE_SUBSET = False  # Trueã«ã™ã‚‹ã¨10%ã®ã¿ä½¿ç”¨

if USE_SUBSET:
    dataset['train'] = dataset['train'].select(range(int(len(dataset['train']) * 0.1)))
    dataset['test'] = dataset['test'].select(range(int(len(dataset['test']) * 0.1)))
    print(f"ã€ã‚µãƒ–ã‚»ãƒƒãƒˆä½¿ç”¨ã€‘")
    print(f"è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset['train']):,}")
    print(f"ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset['test']):,}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 7: SFTTrainerã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

# COMMAND ----------

# DBTITLE 1,SFTTrainerã®ä½œæˆã¨å®Ÿè¡Œ
from trl import SFTTrainer, SFTConfig

# SFTï¼ˆSupervised Fine-Tuningï¼‰è¨­å®š
sft_config = SFTConfig(
    **training_args.to_dict(),
    max_seq_length=1024,                   # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    packing=False,                         # ãƒ‘ãƒƒã‚­ãƒ³ã‚°ç„¡åŠ¹
)

# SFTTrainerã®ä½œæˆ
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    tokenizer=tokenizer,
    peft_config=lora_config,
)

print("âœ… SFTTrainerä½œæˆå®Œäº†")
print("\nã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ã€‘")
print("ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã¯10-30åˆ†ã‹ã‹ã‚Šã¾ã™...\n")

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
trainer.train()

print("\nâœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ“Š ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª
# MAGIC
# MAGIC ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã€ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ï¼š
# MAGIC - **loss**: è¨“ç·´æå¤±ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
# MAGIC - **learning_rate**: ç¾åœ¨ã®å­¦ç¿’ç‡
# MAGIC - **epoch**: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯
# MAGIC
# MAGIC æ­£å¸¸ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€lossãŒå¾ã€…ã«æ¸›å°‘ã—ã¾ã™ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 8: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®æ€§èƒ½è©•ä¾¡

# COMMAND ----------

# DBTITLE 1,ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ†ã‚¹ãƒˆ
print("ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®æ€§èƒ½ã€‘\n")

for i, prompt in enumerate(test_prompts, 1):
    messages = [{"role": "user", "content": prompt}]
    
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    print(f"è³ªå• {i}: {prompt}")
    print(f"å›ç­”: {response}")
    print("-" * 80 + "\n")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹çµæœ
# MAGIC
# MAGIC ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã€ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’ç¤ºã™ã¯ãšã§ã™ï¼š
# MAGIC - **èªå°¾ãŒã€Œã”ã–ã‚‹ã€å£èª¿**ã«ãªã‚‹
# MAGIC - ã‚ˆã‚Šè©³ç´°ã§æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”
# MAGIC - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã«é©å¿œã—ãŸè¡¨ç¾

# COMMAND ----------

# DBTITLE 1,ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”±æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆ
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã—ã¦ãƒ†ã‚¹ãƒˆ
import random

print("ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”±æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆã€‘\n")

for _ in range(3):
    idx = random.randint(0, len(dataset['test']) - 1)
    sample = dataset['test'][idx]['messages']
    
    user_prompt = sample[0]['content']
    expected_output = sample[1]['content']
    
    messages = [{"role": "user", "content": user_prompt}]
    
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    print(f"è³ªå•: {user_prompt[:100]}...")
    print(f"\næœŸå¾…ã•ã‚Œã‚‹å›ç­”: {expected_output[:150]}...")
    print(f"\nãƒ¢ãƒ‡ãƒ«ã®å›ç­”: {response}")
    print("=" * 80 + "\n")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 9: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜

# COMMAND ----------

# DBTITLE 1,ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜ï¼ˆå…ƒã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã¯ä¿å­˜ä¸è¦ï¼‰
adapter_save_dir = "/dbfs/tmp/gemma-3-270m-lora-adapters"

trainer.model.save_pretrained(adapter_save_dir)
tokenizer.save_pretrained(adapter_save_dir)

print(f"âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
print(f"ä¿å­˜å…ˆ: {adapter_save_dir}")
print(f"\nä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")

import os
for file in os.listdir(adapter_save_dir):
    file_path = os.path.join(adapter_save_dir, file)
    if os.path.isfile(file_path):
        size = os.path.getsize(file_path) / 1024**2
        print(f"  {file}: {size:.2f} MB")

# COMMAND ----------

# MAGIC %md
# MAGIC ### ğŸ’¡ LoRAä¿å­˜ã®åˆ©ç‚¹
# MAGIC
# MAGIC - **å…ƒã®ãƒ¢ãƒ‡ãƒ«ï¼ˆ270Mï¼‰**: ç´„540MB
# MAGIC - **LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿**: ç´„5-10MB
# MAGIC - **å‰Šæ¸›ç‡**: ç´„98%
# MAGIC
# MAGIC æ¨è«–æ™‚ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« + LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ä½¿ç”¨å¯èƒ½

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 10: ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨è«–ç”¨ï¼‰

# COMMAND ----------

# DBTITLE 1,ä¿å­˜ã—ãŸLoRAãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
from peft import AutoPeftModelForCausalLM

# æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ
print("ã€ä¿å­˜ã—ãŸLoRAãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã€‘")

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
inference_model = AutoPeftModelForCausalLM.from_pretrained(
    adapter_save_dir,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

inference_tokenizer = AutoTokenizer.from_pretrained(adapter_save_dir)

print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")

# ãƒ†ã‚¹ãƒˆ
test_message = [{"role": "user", "content": "æ·±å±¤å­¦ç¿’ã®åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"}]

inputs = inference_tokenizer.apply_chat_template(
    test_message,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(inference_model.device)

with torch.no_grad():
    outputs = inference_model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        do_sample=True,
        pad_token_id=inference_tokenizer.eos_token_id
    )

generated_ids = outputs[0][inputs["input_ids"].shape[-1]:]
response = inference_tokenizer.decode(generated_ids, skip_special_tokens=True)

print(f"\nã€ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆã€‘")
print(f"è³ªå•: {test_message[0]['content']}")
print(f"å›ç­”: {response}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ Exercise 5ã®ã¾ã¨ã‚
# MAGIC
# MAGIC ã“ã®Exerciseã§å­¦ã‚“ã ã“ã¨ï¼š
# MAGIC
# MAGIC ### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“
# MAGIC 1. **LoRAï¼ˆLow-Rank Adaptationï¼‰**
# MAGIC    - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# MAGIC    - è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’99%ä»¥ä¸Šå‰Šæ¸›
# MAGIC    - å…ƒã®ãƒ¢ãƒ‡ãƒ«é‡ã¿ã¯å‡çµ
# MAGIC
# MAGIC 2. **4-bité‡å­åŒ–**
# MAGIC    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’75%å‰Šæ¸›
# MAGIC    - å°è¦æ¨¡GPUã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«
# MAGIC    - BitsAndBytesConfigã«ã‚ˆã‚‹è¨­å®š
# MAGIC
# MAGIC 3. **Supervised Fine-Tuning (SFT)**
# MAGIC    - TRLã®SFTTrainerã«ã‚ˆã‚‹ç°¡æ½”ãªå®Ÿè£…
# MAGIC    - ãƒãƒ£ãƒƒãƒˆå½¢å¼ãƒ‡ãƒ¼ã‚¿ã§ã®Instruction Tuning
# MAGIC    - è©•ä¾¡ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã®è‡ªå‹•åŒ–
# MAGIC
# MAGIC ### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†
# MAGIC 4. **ãƒãƒ£ãƒƒãƒˆå½¢å¼ã¸ã®å¤‰æ›**
# MAGIC    - Dollyå½¢å¼ã‹ã‚‰OpenAIå½¢å¼ã¸
# MAGIC    - apply_chat_templateã¨ã®çµ±åˆ
# MAGIC
# MAGIC ### ãƒ¢ãƒ‡ãƒ«ç®¡ç†
# MAGIC 5. **åŠ¹ç‡çš„ãªä¿å­˜ã¨ãƒ­ãƒ¼ãƒ‰**
# MAGIC    - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜ï¼ˆç´„5-10MBï¼‰
# MAGIC    - AutoPeftModelForCausalLMã«ã‚ˆã‚‹ãƒ­ãƒ¼ãƒ‰
# MAGIC    - æœ¬ç•ªç’°å¢ƒã¸ã®å®¹æ˜“ãªãƒ‡ãƒ—ãƒ­ã‚¤

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒ
# MAGIC
# MAGIC | é …ç›® | å‰ | å¾Œ |
# MAGIC |------|-----|-----|
# MAGIC | **èªå°¾** | é€šå¸¸ã®æ—¥æœ¬èª | ã€Œã”ã–ã‚‹ã€å£èª¿ |
# MAGIC | **å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«** | æ±ç”¨çš„ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©å¿œ |
# MAGIC | **è©³ç´°åº¦** | ç°¡æ½” | ã‚ˆã‚Šè©³ç´° |
# MAGIC | **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** | 270M | 270M + 0.5Mï¼ˆLoRAï¼‰ |
# MAGIC | **ãƒ¡ãƒ¢ãƒª** | 540MB | 545MB |

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
# MAGIC
# MAGIC ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã‚‰ã€ä»¥ä¸‹ã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼š
# MAGIC
# MAGIC 1. **ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ**
# MAGIC    - è‡ªç¤¾ã®Q&Aãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# MAGIC    - ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆåŒ»ç™‚ã€æ³•å¾‹ã€æŠ€è¡“ï¼‰ã¸ã®ç‰¹åŒ–
# MAGIC
# MAGIC 2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**
# MAGIC    - LoRAãƒ©ãƒ³ã‚¯ï¼ˆrï¼‰ã®æœ€é©åŒ–
# MAGIC    - å­¦ç¿’ç‡ã¨ã‚¨ãƒãƒƒã‚¯æ•°ã®èª¿æ•´
# MAGIC    - target_modulesã®æ‹¡å¼µ
# MAGIC
# MAGIC 3. **è©•ä¾¡æŒ‡æ¨™ã®å®Ÿè£…**
# MAGIC    - ROUGEã€BLEUã€BERTScoreã§ã®è‡ªå‹•è©•ä¾¡
# MAGIC    - äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢åˆ†æ
# MAGIC
# MAGIC 4. **Databricks Model Servingã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤**
# MAGIC    - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåŒ–
# MAGIC    - A/Bãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹æ€§èƒ½æ¯”è¼ƒ
# MAGIC
# MAGIC 5. **ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’**
# MAGIC    - è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ··åˆã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
# MAGIC    - ã‚¿ã‚¹ã‚¯é–“ã®è»¢ç§»å­¦ç¿’åŠ¹æœã®æ¤œè¨¼

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ ç·æ‹¬
# MAGIC
# MAGIC Exercise 5ã§ã¯ã€**LoRAã‚’ä½¿ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã‚’å­¦ã³ã¾ã—ãŸã€‚
# MAGIC
# MAGIC ã“ã‚Œã«ã‚ˆã‚Šï¼š
# MAGIC - å°‘ãªã„ãƒªã‚½ãƒ¼ã‚¹ã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½
# MAGIC - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã‚’å®Ÿç¾
# MAGIC - å®Ÿå‹™ã§ä½¿ãˆã‚‹åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ç®¡ç†æ‰‹æ³•ã‚’ç¿’å¾—
# MAGIC
# MAGIC æ¬¡å›ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¬›ç¾©ã§ã¯ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’
# MAGIC **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**ã¨ã—ã¦æ´»ç”¨ã—ã¾ã™ï¼
